{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Qualitätsfilterung und Korrelations-Prepruning\n",
    "\n",
    "**Masterarbeit:** Zerstörungsfreie Werkstoffprüfung mittels 3MA-X8-Mikromagnetik  \n",
    "**Ziel:** Reduktion von P=261 Features auf ~84 Features\n",
    "\n",
    "---\n",
    "\n",
    "## Methodische Grundlagen\n",
    "\n",
    "### 1.1 Qualitätsfilterung\n",
    "- **Missing Values:** Features mit >15% fehlenden Werten eliminieren\n",
    "- **Near-Zero Variance:** Quasi-konstante Features entfernen\n",
    "\n",
    "### 1.2 One-vs-Rest (OvR) Signal\n",
    "Berechnung überwachter Korrelationen:\n",
    "$$\\text{OvR-Score}_j = \\max_{c \\in \\text{Classes}} \\max(|\\rho_{\\text{Pearson}}|, |\\rho_{\\text{Spearman}}|)$$\n",
    "\n",
    "### 1.3 Hierarchisches Clustering (Redundanzelimination)\n",
    "- **Distanzmetrik:** $d = 1 - |\\rho|$\n",
    "- **Schwellwert:** $|\\rho| \\geq 0.90$ → Cluster bilden\n",
    "- **Repräsentanten:** Hybrid-Score = $0.5 \\cdot \\text{Zentralität} + 0.5 \\cdot \\text{OvR-Signal}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom Utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.validation import validate_data_structure, print_validation_report\n",
    "from utils.visualization import plot_correlation_heatmap\n",
    "\n",
    "# Plotting-Konfiguration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden\n",
    "\n",
    "**WICHTIG:** Passen Sie den Dateipfad an Ihre Datenquelle an!  \n",
    "Erwartete Struktur:\n",
    "- **Spalten:** 261 Features + 1 Zielvariable (z.B. 'class') + 1 Proben-ID (z.B. 'sample_id')\n",
    "- **Zeilen:** n Samples (typischerweise Mehrfachmessungen pro Probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANPASSEN: Dateipfad zu Ihren 3MA-X8 Daten\n",
    "# ============================================================================\n",
    "DATA_PATH = '../data/raw/3ma_x8_features.csv'  # <-- HIER ANPASSEN!\n",
    "\n",
    "# Daten einlesen\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"✓ Daten geladen: {df_raw.shape}\")\n",
    "print(f\"\\nErste 5 Zeilen:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANPASSEN: Spalten-Namen für Zielvariable und Proben-ID\n",
    "# ============================================================================\n",
    "TARGET_COL = 'class'       # <-- Name der Zielvariablen-Spalte\n",
    "GROUP_COL = 'sample_id'    # <-- Name der Proben-ID-Spalte\n",
    "\n",
    "# Feature-Matrix X, Zielvariable y, Gruppen\n",
    "feature_cols = [col for col in df_raw.columns if col not in [TARGET_COL, GROUP_COL]]\n",
    "X = df_raw[feature_cols].copy()\n",
    "y = df_raw[TARGET_COL].copy()\n",
    "groups = df_raw[GROUP_COL].copy()\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Klassen: {y.nunique()} ({y.value_counts().to_dict()})\")\n",
    "print(f\"Gruppen (Proben): {groups.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datenstruktur-Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validierung gegen Spezifikation\n",
    "validation_results = validate_data_structure(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    groups=groups,\n",
    "    expected_features=261\n",
    ")\n",
    "\n",
    "print_validation_report(validation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schritt 1.1: Missing Values Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_THRESHOLD = 0.15  # 15% gemäß Spezifikation\n",
    "\n",
    "# Berechne Anteil fehlender Werte pro Feature\n",
    "missing_ratios = X.isnull().mean()\n",
    "features_to_keep = missing_ratios[missing_ratios <= MISSING_THRESHOLD].index.tolist()\n",
    "\n",
    "n_removed = len(X.columns) - len(features_to_keep)\n",
    "print(f\"✓ Missing Values Filter:\")\n",
    "print(f\"  Features entfernt: {n_removed}\")\n",
    "print(f\"  Features verbleibend: {len(features_to_keep)}\")\n",
    "\n",
    "X_filtered = X[features_to_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Schritt 1.2: Near-Zero Variance Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Varianz-Schwellwert (relativ zur Spannweite)\n",
    "VARIANCE_THRESHOLD = 0.01\n",
    "\n",
    "# Standardisiere Features temporär für Varianzberechnung\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_temp = StandardScaler()\n",
    "X_scaled_temp = scaler_temp.fit_transform(X_filtered)\n",
    "\n",
    "# VarianceThreshold anwenden\n",
    "selector = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "selector.fit(X_scaled_temp)\n",
    "\n",
    "features_high_var = X_filtered.columns[selector.get_support()].tolist()\n",
    "\n",
    "n_removed_var = len(X_filtered.columns) - len(features_high_var)\n",
    "print(f\"✓ Near-Zero Variance Filter:\")\n",
    "print(f\"  Features entfernt: {n_removed_var}\")\n",
    "print(f\"  Features verbleibend: {len(features_high_var)}\")\n",
    "\n",
    "X_filtered = X_filtered[features_high_var].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Schritt 1.3: One-vs-Rest (OvR) Signal berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ovr_signal(X, y):\n",
    "    \"\"\"\n",
    "    Berechnet OvR-Signal für jedes Feature:\n",
    "    max_over_classes(max(|ρ_Pearson|, |ρ_Spearman|))\n",
    "    \"\"\"\n",
    "    # Binäre Klassen-Indikatoren\n",
    "    lb = LabelBinarizer()\n",
    "    y_binary = lb.fit_transform(y)\n",
    "    if y_binary.shape[1] == 1:  # Falls nur 2 Klassen\n",
    "        y_binary = np.hstack([1 - y_binary, y_binary])\n",
    "    \n",
    "    ovr_scores = {}\n",
    "    \n",
    "    for feature in X.columns:\n",
    "        feature_values = X[feature].values\n",
    "        \n",
    "        # Missing Values temporär durch Median ersetzen\n",
    "        if np.isnan(feature_values).any():\n",
    "            feature_values = pd.Series(feature_values).fillna(pd.Series(feature_values).median()).values\n",
    "        \n",
    "        max_corr = 0\n",
    "        \n",
    "        # Für jede Klasse\n",
    "        for class_idx in range(y_binary.shape[1]):\n",
    "            class_indicator = y_binary[:, class_idx]\n",
    "            \n",
    "            # Pearson\n",
    "            try:\n",
    "                corr_p, _ = pearsonr(feature_values, class_indicator)\n",
    "                corr_p = abs(corr_p) if not np.isnan(corr_p) else 0\n",
    "            except:\n",
    "                corr_p = 0\n",
    "            \n",
    "            # Spearman\n",
    "            try:\n",
    "                corr_s, _ = spearmanr(feature_values, class_indicator)\n",
    "                corr_s = abs(corr_s) if not np.isnan(corr_s) else 0\n",
    "            except:\n",
    "                corr_s = 0\n",
    "            \n",
    "            # Max über beide Korrelationen\n",
    "            max_corr = max(max_corr, corr_p, corr_s)\n",
    "        \n",
    "        ovr_scores[feature] = max_corr\n",
    "    \n",
    "    return pd.Series(ovr_scores)\n",
    "\n",
    "# Berechnung\n",
    "print(\"Berechne OvR-Signal für {} Features...\".format(len(X_filtered.columns)))\n",
    "ovr_signal = compute_ovr_signal(X_filtered, y)\n",
    "\n",
    "print(f\"✓ OvR-Signal berechnet\")\n",
    "print(f\"  Mittelwert: {ovr_signal.mean():.3f}\")\n",
    "print(f\"  Median: {ovr_signal.median():.3f}\")\n",
    "print(f\"\\nTop 10 Features (höchstes OvR-Signal):\")\n",
    "print(ovr_signal.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Schritt 1.4: Hierarchisches Clustering & Redundanzelimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrelationsmatrix berechnen (Pearson)\n",
    "X_clean = X_filtered.fillna(X_filtered.median())  # Temporäre Imputation für Korrelationsberechnung\n",
    "corr_matrix = X_clean.corr(method='pearson')\n",
    "\n",
    "print(f\"✓ Korrelationsmatrix: {corr_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distanzmatrix: d = 1 - |ρ|\n",
    "distance_matrix = 1 - np.abs(corr_matrix.values)\n",
    "\n",
    "# Hierarchisches Clustering (Average Linkage)\n",
    "from scipy.spatial.distance import squareform\n",
    "condensed_dist = squareform(distance_matrix, checks=False)\n",
    "Z = linkage(condensed_dist, method='average')\n",
    "\n",
    "# Cluster bilden bei |ρ| ≥ 0.90 → Distanz ≤ 0.10\n",
    "CORR_THRESHOLD = 0.90\n",
    "DISTANCE_THRESHOLD = 1 - CORR_THRESHOLD\n",
    "\n",
    "cluster_labels = fcluster(Z, t=DISTANCE_THRESHOLD, criterion='distance')\n",
    "\n",
    "print(f\"✓ Clustering abgeschlossen\")\n",
    "print(f\"  Anzahl Cluster: {len(np.unique(cluster_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repräsentanten-Auswahl pro Cluster\n",
    "def select_cluster_representatives(features, cluster_labels, corr_matrix, ovr_signal, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Wählt pro Cluster den Repräsentanten mit höchstem Hybrid-Score.\n",
    "    \n",
    "    Hybrid-Score = α * Zentralität + (1-α) * OvR-Signal\n",
    "    \n",
    "    Zentralität = mittlere absolute Korrelation zu allen Cluster-Mitgliedern\n",
    "    \"\"\"\n",
    "    representatives = []\n",
    "    \n",
    "    for cluster_id in np.unique(cluster_labels):\n",
    "        # Features im Cluster\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_features = np.array(features)[cluster_mask]\n",
    "        \n",
    "        if len(cluster_features) == 1:\n",
    "            # Singleton-Cluster\n",
    "            representatives.append(cluster_features[0])\n",
    "        else:\n",
    "            # Berechne Hybrid-Score für jedes Feature im Cluster\n",
    "            scores = {}\n",
    "            \n",
    "            for feat in cluster_features:\n",
    "                # Zentralität: mittlere |ρ| zu allen anderen im Cluster\n",
    "                feat_idx = features.tolist().index(feat)\n",
    "                cluster_indices = np.where(cluster_mask)[0]\n",
    "                centrality = np.abs(corr_matrix.iloc[feat_idx, cluster_indices]).mean()\n",
    "                \n",
    "                # OvR-Signal normalisieren auf [0,1]\n",
    "                ovr_normalized = ovr_signal[feat]\n",
    "                \n",
    "                # Hybrid-Score\n",
    "                hybrid_score = alpha * centrality + (1 - alpha) * ovr_normalized\n",
    "                scores[feat] = hybrid_score\n",
    "            \n",
    "            # Bester Repräsentant\n",
    "            best_rep = max(scores, key=scores.get)\n",
    "            representatives.append(best_rep)\n",
    "    \n",
    "    return representatives\n",
    "\n",
    "# Repräsentanten auswählen\n",
    "features_array = X_filtered.columns.values\n",
    "representatives = select_cluster_representatives(\n",
    "    features=features_array,\n",
    "    cluster_labels=cluster_labels,\n",
    "    corr_matrix=corr_matrix,\n",
    "    ovr_signal=ovr_signal,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "print(f\"✓ Repräsentanten ausgewählt\")\n",
    "print(f\"  Features nach Clustering: {len(representatives)}\")\n",
    "print(f\"  Reduktionsrate: {(1 - len(representatives) / len(X_filtered.columns)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finale Feature-Matrix Phase 1\n",
    "X_phase1 = X_filtered[representatives].copy()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 1 ABGESCHLOSSEN\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Features vorher:  {X.shape[1]}\")\n",
    "print(f\"Features nachher: {X_phase1.shape[1]}\")\n",
    "print(f\"Reduktion:        {X.shape[1] - X_phase1.shape[1]} Features ({(1 - X_phase1.shape[1] / X.shape[1]) * 100:.1f}%)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisierung: Korrelations-Heatmap (Top 30 Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Korrelationsheatmap der Top 30 Features (sortiert nach OvR-Signal)\n",
    "top_30_features = ovr_signal.loc[representatives].sort_values(ascending=False).head(30).index.tolist()\n",
    "\n",
    "fig = plot_correlation_heatmap(\n",
    "    X=X_phase1[top_30_features],\n",
    "    method='pearson',\n",
    "    top_k=30,\n",
    "    save_path='../results/plots/phase1_correlation_heatmap.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichere reduzierte Feature-Matrix\n",
    "output_df = X_phase1.copy()\n",
    "output_df[TARGET_COL] = y\n",
    "output_df[GROUP_COL] = groups\n",
    "\n",
    "output_path = '../data/processed/features_after_phase1.csv'\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"✓ Daten gespeichert: {output_path}\")\n",
    "\n",
    "# Speichere Feature-Liste und OvR-Scores\n",
    "feature_info = pd.DataFrame({\n",
    "    'feature': representatives,\n",
    "    'ovr_signal': [ovr_signal[f] for f in representatives]\n",
    "}).sort_values('ovr_signal', ascending=False)\n",
    "\n",
    "feature_info_path = '../results/rankings/phase1_feature_info.csv'\n",
    "feature_info.to_csv(feature_info_path, index=False)\n",
    "print(f\"✓ Feature-Info gespeichert: {feature_info_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✓ Phase 1 abgeschlossen!\n",
    "\n",
    "**Nächster Schritt:** Notebook 2 - Phase 2: Multi-Methoden Feature-Ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
