{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Iterative Reduktions-Evaluierung (LDA/QDA Benchmarking)\n",
    "\n",
    "**Masterarbeit:** Zerstörungsfreie Werkstoffprüfung mittels 3MA-X8-Mikromagnetik  \n",
    "**Input:** 8 Feature-Rankings aus Phase 2  \n",
    "**Output:** Pareto-Kurven (Feature-Anzahl vs. Performance)\n",
    "\n",
    "---\n",
    "\n",
    "## Methodische Grundlagen\n",
    "\n",
    "### Evaluierungsprotokoll\n",
    "\n",
    "1. **Reduktionsstufen:** 10 Stufen pro Ranking  \n",
    "   → 90%, 80%, 70%, 60%, 50%, 40%, 30%, 20%, 10%, 5% der ~84 Features\n",
    "\n",
    "2. **Klassifikatoren:**\n",
    "   - **LDA (Linear Discriminant Analysis):** Annahme gemeinsamer Kovarianzmatrizen\n",
    "   - **QDA (Quadratic Discriminant Analysis):** Klassenspezifische Kovarianzmatrizen\n",
    "\n",
    "3. **Validierung:** 5-Fold GroupKFold CV  \n",
    "   → Preprocessing (Imputation + Scaling) **innerhalb** jedes Folds!\n",
    "\n",
    "4. **Metriken (Priorität):**\n",
    "   1. Balanced Accuracy\n",
    "   2. F1-Score (macro)\n",
    "   3. Cohen's Kappa\n",
    "   4. Standard Accuracy\n",
    "\n",
    "5. **Konfidenzintervalle:** 95% CI via t-Verteilung (df=4)\n",
    "\n",
    "### KRITISCHE WARNUNG\n",
    "\n",
    "**QDA ist extrem anfällig für Overfitting bei kleinen Stichproben!**  \n",
    "Mit typischerweise nur 2-3 Samples pro Klasse im Training-Fold (bei 12 Klassen) sind instabile Parameterschätzungen zu erwarten.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom Utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.validation import create_group_kfold_splits, calculate_confidence_intervals\n",
    "from utils.metrics import compute_classification_metrics, aggregate_cv_metrics\n",
    "from utils.visualization import plot_pareto_curve, plot_classifier_comparison\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden (Output von Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten\n",
    "DATA_PATH = '../data/processed/features_after_phase1.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "TARGET_COL = 'class'\n",
    "GROUP_COL = 'sample_id'\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in [TARGET_COL, GROUP_COL]]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "groups = df[GROUP_COL].copy()\n",
    "\n",
    "print(f\"✓ Daten geladen: {X.shape}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Klassen: {y.nunique()}\")\n",
    "print(f\"  Gruppen: {groups.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rankings laden (Output von Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Lade alle Rankings\n",
    "ranking_files = glob.glob('../results/rankings/phase2_ranking_*.csv')\n",
    "\n",
    "rankings = {}\n",
    "for file_path in ranking_files:\n",
    "    method_name = os.path.basename(file_path).replace('phase2_ranking_', '').replace('.csv', '')\n",
    "    rankings[method_name] = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"✓ {len(rankings)} Rankings geladen:\")\n",
    "for method_name in rankings.keys():\n",
    "    print(f\"  - {method_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reduktionsstufen definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Reduktionsstufen (Prozentuale Anteile)\n",
    "REDUCTION_PERCENTAGES = [0.90, 0.80, 0.70, 0.60, 0.50, 0.40, 0.30, 0.20, 0.10, 0.05]\n",
    "\n",
    "# Anzahl Features pro Stufe\n",
    "n_total_features = X.shape[1]\n",
    "reduction_steps = [max(1, int(n_total_features * p)) for p in REDUCTION_PERCENTAGES]\n",
    "\n",
    "print(\"Reduktionsstufen:\")\n",
    "for pct, n_feat in zip(REDUCTION_PERCENTAGES, reduction_steps):\n",
    "    print(f\"  {int(pct*100):2d}%: {n_feat:3d} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluierungs-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_subset(\n",
    "    X, y, groups,\n",
    "    feature_subset,\n",
    "    classifier_name='LDA',\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluiert ein Feature-Subset mit LDA oder QDA via GroupKFold CV.\n",
    "    \n",
    "    KRITISCH: Preprocessing innerhalb jedes Folds!\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Aggregierte Metriken mit CI\n",
    "    \"\"\"\n",
    "    # Subset-Daten\n",
    "    X_subset = X[feature_subset].copy()\n",
    "    \n",
    "    # Klassifikator\n",
    "    if classifier_name == 'LDA':\n",
    "        clf_base = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    elif classifier_name == 'QDA':\n",
    "        clf_base = QuadraticDiscriminantAnalysis(reg_param=0.1)  # Regularisierung!\n",
    "    else:\n",
    "        raise ValueError(f\"Unbekannter Klassifikator: {classifier_name}\")\n",
    "    \n",
    "    # Pipeline: Imputation → Scaling → Klassifikator\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf_base)\n",
    "    ])\n",
    "    \n",
    "    # GroupKFold\n",
    "    gkf = create_group_kfold_splits(n_splits=n_splits)\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X_subset, y, groups):\n",
    "        X_train, X_test = X_subset.iloc[train_idx], X_subset.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        try:\n",
    "            # Fit Pipeline (Preprocessing + Classifier)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Metriken\n",
    "            fold_metrics = compute_classification_metrics(y_test, y_pred)\n",
    "            cv_results.append(fold_metrics)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Bei Fehler (z.B. QDA Singularität): NaN-Metriken\n",
    "            cv_results.append({\n",
    "                'balanced_accuracy': np.nan,\n",
    "                'f1_macro': np.nan,\n",
    "                'cohen_kappa': np.nan,\n",
    "                'accuracy': np.nan\n",
    "            })\n",
    "    \n",
    "    # Aggregation\n",
    "    aggregated = aggregate_cv_metrics(cv_results)\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "\n",
    "print(\"✓ Evaluierungs-Pipeline definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Haupt-Evaluierungs-Loop\n",
    "\n",
    "**WARNUNG:** Dies kann mehrere Minuten dauern (8 Rankings × 10 Stufen × 2 Klassifikatoren × 5 Folds = 800 Trainings)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage für Ergebnisse\n",
    "results_master = []\n",
    "\n",
    "# Loop über Rankings\n",
    "for method_name, ranking_df in tqdm(rankings.items(), desc=\"Methoden\"):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluiere: {method_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Loop über Reduktionsstufen\n",
    "    for k_features in tqdm(reduction_steps, desc=\"Reduktionsstufen\", leave=False):\n",
    "        # Top-K Features\n",
    "        top_k_features = ranking_df.head(k_features)['feature'].tolist()\n",
    "        \n",
    "        # Evaluierung mit LDA\n",
    "        try:\n",
    "            lda_metrics = evaluate_feature_subset(\n",
    "                X=X, y=y, groups=groups,\n",
    "                feature_subset=top_k_features,\n",
    "                classifier_name='LDA',\n",
    "                n_splits=5\n",
    "            )\n",
    "        except:\n",
    "            lda_metrics = None\n",
    "        \n",
    "        # Evaluierung mit QDA\n",
    "        try:\n",
    "            qda_metrics = evaluate_feature_subset(\n",
    "                X=X, y=y, groups=groups,\n",
    "                feature_subset=top_k_features,\n",
    "                classifier_name='QDA',\n",
    "                n_splits=5\n",
    "            )\n",
    "        except:\n",
    "            qda_metrics = None\n",
    "        \n",
    "        # Speichere Ergebnisse\n",
    "        result_row = {\n",
    "            'method': method_name,\n",
    "            'n_features': k_features,\n",
    "            'pct_features': k_features / n_total_features\n",
    "        }\n",
    "        \n",
    "        # LDA Metriken\n",
    "        if lda_metrics is not None:\n",
    "            for metric in ['balanced_accuracy', 'f1_macro', 'cohen_kappa', 'accuracy']:\n",
    "                if metric in lda_metrics.index:\n",
    "                    result_row[f'LDA_{metric}_mean'] = lda_metrics.loc[metric, 'mean']\n",
    "                    result_row[f'LDA_{metric}_std'] = lda_metrics.loc[metric, 'std']\n",
    "                    result_row[f'LDA_{metric}_ci_lower'] = lda_metrics.loc[metric, 'ci_lower']\n",
    "                    result_row[f'LDA_{metric}_ci_upper'] = lda_metrics.loc[metric, 'ci_upper']\n",
    "        \n",
    "        # QDA Metriken\n",
    "        if qda_metrics is not None:\n",
    "            for metric in ['balanced_accuracy', 'f1_macro', 'cohen_kappa', 'accuracy']:\n",
    "                if metric in qda_metrics.index:\n",
    "                    result_row[f'QDA_{metric}_mean'] = qda_metrics.loc[metric, 'mean']\n",
    "                    result_row[f'QDA_{metric}_std'] = qda_metrics.loc[metric, 'std']\n",
    "                    result_row[f'QDA_{metric}_ci_lower'] = qda_metrics.loc[metric, 'ci_lower']\n",
    "                    result_row[f'QDA_{metric}_ci_upper'] = qda_metrics.loc[metric, 'ci_upper']\n",
    "        \n",
    "        results_master.append(result_row)\n",
    "\n",
    "# Results DataFrame\n",
    "results_df = pd.DataFrame(results_master)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ EVALUIERUNG ABGESCHLOSSEN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Gesamt-Evaluierungen: {len(results_df)}\")\n",
    "print(f\"Methoden: {results_df['method'].nunique()}\")\n",
    "print(f\"Reduktionsstufen: {results_df['n_features'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ergebnisse anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: ANOVA-Ranking\n",
    "if 'ANOVA' in results_df['method'].values:\n",
    "    anova_results = results_df[results_df['method'] == 'ANOVA'].sort_values('n_features', ascending=False)\n",
    "    \n",
    "    print(\"\\nANOVA-Ranking - LDA Performance:\")\n",
    "    print(anova_results[['n_features', 'LDA_balanced_accuracy_mean', 'LDA_balanced_accuracy_ci_lower', 'LDA_balanced_accuracy_ci_upper']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nANOVA-Ranking - QDA Performance:\")\n",
    "    print(anova_results[['n_features', 'QDA_balanced_accuracy_mean', 'QDA_balanced_accuracy_ci_lower', 'QDA_balanced_accuracy_ci_upper']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisierung: Pareto-Kurven\n",
    "\n",
    "### 7.1 LDA Pareto-Kurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereite Daten für Pareto-Plot vor (LDA)\n",
    "pareto_data_lda = results_df[['method', 'n_features', 'LDA_balanced_accuracy_mean', \n",
    "                               'LDA_balanced_accuracy_ci_lower', 'LDA_balanced_accuracy_ci_upper']].copy()\n",
    "pareto_data_lda.columns = ['method', 'n_features', 'mean', 'ci_lower', 'ci_upper']\n",
    "pareto_data_lda = pareto_data_lda.dropna()\n",
    "\n",
    "# Plot\n",
    "fig_lda = plot_pareto_curve(\n",
    "    results_df=pareto_data_lda,\n",
    "    metric='balanced_accuracy',\n",
    "    classifier='LDA',\n",
    "    save_path='../results/plots/phase3_pareto_lda.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 QDA Pareto-Kurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereite Daten für Pareto-Plot vor (QDA)\n",
    "pareto_data_qda = results_df[['method', 'n_features', 'QDA_balanced_accuracy_mean', \n",
    "                               'QDA_balanced_accuracy_ci_lower', 'QDA_balanced_accuracy_ci_upper']].copy()\n",
    "pareto_data_qda.columns = ['method', 'n_features', 'mean', 'ci_lower', 'ci_upper']\n",
    "pareto_data_qda = pareto_data_qda.dropna()\n",
    "\n",
    "# Plot\n",
    "fig_qda = plot_pareto_curve(\n",
    "    results_df=pareto_data_qda,\n",
    "    metric='balanced_accuracy',\n",
    "    classifier='QDA',\n",
    "    save_path='../results/plots/phase3_pareto_qda.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 LDA vs. QDA Vergleich (Beste Methode bei 50% Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde besten Punkt bei 50% Features\n",
    "target_pct = 0.50\n",
    "subset_50 = results_df[np.abs(results_df['pct_features'] - target_pct) < 0.05].copy()\n",
    "\n",
    "if len(subset_50) > 0:\n",
    "    # Beste Methode für LDA\n",
    "    best_lda = subset_50.loc[subset_50['LDA_balanced_accuracy_mean'].idxmax()]\n",
    "    print(f\"Beste LDA-Performance bei ~50% Features:\")\n",
    "    print(f\"  Methode: {best_lda['method']}\")\n",
    "    print(f\"  Features: {best_lda['n_features']}\")\n",
    "    print(f\"  Balanced Accuracy: {best_lda['LDA_balanced_accuracy_mean']:.3f} \"\n",
    "          f\"[{best_lda['LDA_balanced_accuracy_ci_lower']:.3f}, {best_lda['LDA_balanced_accuracy_ci_upper']:.3f}]\")\n",
    "    \n",
    "    # Beste Methode für QDA\n",
    "    best_qda = subset_50.loc[subset_50['QDA_balanced_accuracy_mean'].idxmax()]\n",
    "    print(f\"\\nBeste QDA-Performance bei ~50% Features:\")\n",
    "    print(f\"  Methode: {best_qda['method']}\")\n",
    "    print(f\"  Features: {best_qda['n_features']}\")\n",
    "    print(f\"  Balanced Accuracy: {best_qda['QDA_balanced_accuracy_mean']:.3f} \"\n",
    "          f\"[{best_qda['QDA_balanced_accuracy_ci_lower']:.3f}, {best_qda['QDA_balanced_accuracy_ci_upper']:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results/evaluations', exist_ok=True)\n",
    "\n",
    "# Master-Tabelle\n",
    "output_path = '../results/evaluations/phase3_evaluation_master.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"✓ Evaluierungs-Ergebnisse gespeichert: {output_path}\")\n",
    "\n",
    "# Kompakte Version (nur Balanced Accuracy)\n",
    "compact_cols = ['method', 'n_features', 'pct_features',\n",
    "                'LDA_balanced_accuracy_mean', 'LDA_balanced_accuracy_ci_lower', 'LDA_balanced_accuracy_ci_upper',\n",
    "                'QDA_balanced_accuracy_mean', 'QDA_balanced_accuracy_ci_lower', 'QDA_balanced_accuracy_ci_upper']\n",
    "compact_df = results_df[compact_cols].copy()\n",
    "\n",
    "compact_path = '../results/evaluations/phase3_evaluation_compact.csv'\n",
    "compact_df.to_csv(compact_path, index=False)\n",
    "print(f\"✓ Kompakte Evaluierung gespeichert: {compact_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✓ Phase 3 abgeschlossen!\n",
    "\n",
    "**Nächster Schritt:** Notebook 4 - Phase 4: Konsensus-Analyse und finales Ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
