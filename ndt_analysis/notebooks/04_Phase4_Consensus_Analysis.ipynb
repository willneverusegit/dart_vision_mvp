{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Konsensus-Analyse und Finales Ranking\n",
    "\n",
    "**Masterarbeit:** Zerstörungsfreie Werkstoffprüfung mittels 3MA-X8-Mikromagnetik  \n",
    "**Input:** 8 Feature-Rankings aus Phase 2  \n",
    "**Output:** Methodenagnostisches Core-Set robuster Features\n",
    "\n",
    "---\n",
    "\n",
    "## Methodische Grundlagen\n",
    "\n",
    "### Konsensus-Score Berechnung\n",
    "\n",
    "1. **Rang-Normalisierung:**  \n",
    "   Für jede Methode $m$ und Feature $f$:\n",
    "   $$\\text{norm\\_rank}_{m,f} = 1 - \\frac{\\text{rank}_{m,f} - 1}{N_{\\text{features}} - 1}$$\n",
    "   \n",
    "   → Rang 1 → Score 1.0  \n",
    "   → Rang N → Score 0.0\n",
    "\n",
    "2. **Konsensus-Score (ungewichtet):**  \n",
    "   $$\\text{Consensus}_f = \\frac{1}{M} \\sum_{m=1}^{M} \\text{norm\\_rank}_{m,f}$$\n",
    "   \n",
    "   wobei $M = 8$ (Anzahl Methoden)\n",
    "\n",
    "3. **Robustheit-Metrik:**  \n",
    "   $$\\text{Rank\\_Variance}_f = \\text{Var}(\\text{rank}_{1,f}, \\ldots, \\text{rank}_{M,f})$$\n",
    "   \n",
    "   → Niedrige Varianz = hohe Übereinstimmung zwischen Methoden\n",
    "\n",
    "### Validierung\n",
    "\n",
    "Das finale Konsensus-Ranking wird separat mit LDA/QDA evaluiert (10 Stufen, GroupKFold).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom Utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.validation import create_group_kfold_splits\n",
    "from utils.metrics import compute_classification_metrics, aggregate_cv_metrics, compare_classifiers\n",
    "from utils.visualization import plot_pareto_curve, plot_feature_reduction_timeline\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rankings laden (Output von Phase 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Lade alle Rankings\n",
    "ranking_files = glob.glob('../results/rankings/phase2_ranking_*.csv')\n",
    "\n",
    "rankings = {}\n",
    "for file_path in ranking_files:\n",
    "    method_name = os.path.basename(file_path).replace('phase2_ranking_', '').replace('.csv', '')\n",
    "    rankings[method_name] = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"✓ {len(rankings)} Rankings geladen:\")\n",
    "for method_name, ranking_df in rankings.items():\n",
    "    print(f\"  - {method_name}: {len(ranking_df)} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konsensus-Score Berechnung\n",
    "\n",
    "### 2.1 Rang-Normalisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ranks(ranking_df, rank_col='final_rank'):\n",
    "    \"\"\"\n",
    "    Normalisiert Ränge auf [0, 1].\n",
    "    \n",
    "    Rang 1 → 1.0 (beste)\n",
    "    Rang N → 0.0 (schlechteste)\n",
    "    \"\"\"\n",
    "    ranks = ranking_df[rank_col].values\n",
    "    n_features = len(ranks)\n",
    "    \n",
    "    # Normalisierung\n",
    "    normalized = 1 - (ranks - 1) / (n_features - 1)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# Normalisiere alle Rankings\n",
    "normalized_rankings = {}\n",
    "\n",
    "for method_name, ranking_df in rankings.items():\n",
    "    df_norm = ranking_df.copy()\n",
    "    df_norm['normalized_score'] = normalize_ranks(df_norm, rank_col='final_rank')\n",
    "    normalized_rankings[method_name] = df_norm\n",
    "\n",
    "print(\"✓ Ränge normalisiert\")\n",
    "\n",
    "# Beispiel: ANOVA\n",
    "if 'ANOVA' in normalized_rankings:\n",
    "    print(\"\\nBeispiel (ANOVA, Top 10):\")\n",
    "    print(normalized_rankings['ANOVA'][['feature', 'final_rank', 'normalized_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Konsensus-Score (Mittelwert über Methoden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammle alle Features\n",
    "all_features = set()\n",
    "for ranking_df in normalized_rankings.values():\n",
    "    all_features.update(ranking_df['feature'].tolist())\n",
    "\n",
    "all_features = sorted(list(all_features))\n",
    "print(f\"✓ Gesamt-Features: {len(all_features)}\")\n",
    "\n",
    "# Konsensus-Datenstruktur\n",
    "consensus_data = []\n",
    "\n",
    "for feature in all_features:\n",
    "    feature_scores = []\n",
    "    feature_ranks = []\n",
    "    \n",
    "    for method_name, ranking_df in normalized_rankings.items():\n",
    "        # Finde Feature in Ranking\n",
    "        feature_row = ranking_df[ranking_df['feature'] == feature]\n",
    "        \n",
    "        if len(feature_row) > 0:\n",
    "            feature_scores.append(feature_row.iloc[0]['normalized_score'])\n",
    "            feature_ranks.append(feature_row.iloc[0]['final_rank'])\n",
    "        else:\n",
    "            # Falls Feature fehlt (sollte nicht passieren)\n",
    "            feature_scores.append(0.0)\n",
    "            feature_ranks.append(len(all_features))\n",
    "    \n",
    "    # Konsensus-Score (Mittelwert)\n",
    "    consensus_score = np.mean(feature_scores)\n",
    "    \n",
    "    # Rang-Varianz (Robustheit)\n",
    "    rank_variance = np.var(feature_ranks, ddof=1)\n",
    "    rank_std = np.std(feature_ranks, ddof=1)\n",
    "    \n",
    "    consensus_data.append({\n",
    "        'feature': feature,\n",
    "        'consensus_score': consensus_score,\n",
    "        'rank_variance': rank_variance,\n",
    "        'rank_std': rank_std,\n",
    "        'mean_rank': np.mean(feature_ranks),\n",
    "        'median_rank': np.median(feature_ranks)\n",
    "    })\n",
    "\n",
    "# Konsensus-DataFrame\n",
    "consensus_df = pd.DataFrame(consensus_data)\n",
    "consensus_df = consensus_df.sort_values('consensus_score', ascending=False).reset_index(drop=True)\n",
    "consensus_df['consensus_rank'] = range(1, len(consensus_df) + 1)\n",
    "\n",
    "print(\"\\n✓ Konsensus-Ranking erstellt\")\n",
    "print(f\"\\nTop 20 Features (Konsensus):\")\n",
    "print(consensus_df.head(20)[['consensus_rank', 'feature', 'consensus_score', 'rank_std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Robustheitsbewertung\n",
    "\n",
    "Features mit niedriger Rang-Varianz sind über Methoden hinweg konsistent hoch bewertet → hohe Robustheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 robusteste Features (niedrigste Rang-Standardabweichung)\n",
    "robust_features = consensus_df.nsmallest(20, 'rank_std')\n",
    "\n",
    "print(\"Top 20 robusteste Features (niedrigste Rang-Varianz):\")\n",
    "print(robust_features[['feature', 'consensus_rank', 'consensus_score', 'rank_std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Konsensus-Ranking Validierung\n",
    "\n",
    "Evaluiere das Konsensus-Ranking mit LDA/QDA (10 Reduktionsstufen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "DATA_PATH = '../data/processed/features_after_phase1.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "TARGET_COL = 'class'\n",
    "GROUP_COL = 'sample_id'\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in [TARGET_COL, GROUP_COL]]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "groups = df[GROUP_COL].copy()\n",
    "\n",
    "print(f\"✓ Daten geladen: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluierungs-Funktion (aus Phase 3)\n",
    "def evaluate_feature_subset(\n",
    "    X, y, groups,\n",
    "    feature_subset,\n",
    "    classifier_name='LDA',\n",
    "    n_splits=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluiert Feature-Subset mit LDA oder QDA via GroupKFold CV.\n",
    "    \"\"\"\n",
    "    X_subset = X[feature_subset].copy()\n",
    "    \n",
    "    if classifier_name == 'LDA':\n",
    "        clf_base = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    elif classifier_name == 'QDA':\n",
    "        clf_base = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unbekannter Klassifikator: {classifier_name}\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf_base)\n",
    "    ])\n",
    "    \n",
    "    gkf = create_group_kfold_splits(n_splits=n_splits)\n",
    "    \n",
    "    cv_results = []\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X_subset, y, groups):\n",
    "        X_train, X_test = X_subset.iloc[train_idx], X_subset.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        try:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            fold_metrics = compute_classification_metrics(y_test, y_pred)\n",
    "            cv_results.append(fold_metrics)\n",
    "        except:\n",
    "            cv_results.append({\n",
    "                'balanced_accuracy': np.nan,\n",
    "                'f1_macro': np.nan,\n",
    "                'cohen_kappa': np.nan,\n",
    "                'accuracy': np.nan\n",
    "            })\n",
    "    \n",
    "    aggregated = aggregate_cv_metrics(cv_results)\n",
    "    return aggregated\n",
    "\n",
    "print(\"✓ Evaluierungs-Funktion definiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduktionsstufen\n",
    "REDUCTION_PERCENTAGES = [0.90, 0.80, 0.70, 0.60, 0.50, 0.40, 0.30, 0.20, 0.10, 0.05]\n",
    "n_total_features = len(consensus_df)\n",
    "reduction_steps = [max(1, int(n_total_features * p)) for p in REDUCTION_PERCENTAGES]\n",
    "\n",
    "# Evaluierung\n",
    "consensus_results = []\n",
    "\n",
    "print(\"Evaluiere Konsensus-Ranking...\\n\")\n",
    "\n",
    "for k_features in tqdm(reduction_steps, desc=\"Reduktionsstufen\"):\n",
    "    # Top-K Features\n",
    "    top_k_features = consensus_df.head(k_features)['feature'].tolist()\n",
    "    \n",
    "    # LDA\n",
    "    try:\n",
    "        lda_metrics = evaluate_feature_subset(\n",
    "            X=X, y=y, groups=groups,\n",
    "            feature_subset=top_k_features,\n",
    "            classifier_name='LDA',\n",
    "            n_splits=5\n",
    "        )\n",
    "    except:\n",
    "        lda_metrics = None\n",
    "    \n",
    "    # QDA\n",
    "    try:\n",
    "        qda_metrics = evaluate_feature_subset(\n",
    "            X=X, y=y, groups=groups,\n",
    "            feature_subset=top_k_features,\n",
    "            classifier_name='QDA',\n",
    "            n_splits=5\n",
    "        )\n",
    "    except:\n",
    "        qda_metrics = None\n",
    "    \n",
    "    # Speichere\n",
    "    result_row = {\n",
    "        'method': 'Consensus',\n",
    "        'n_features': k_features,\n",
    "        'pct_features': k_features / n_total_features\n",
    "    }\n",
    "    \n",
    "    if lda_metrics is not None:\n",
    "        for metric in ['balanced_accuracy', 'f1_macro', 'cohen_kappa']:\n",
    "            if metric in lda_metrics.index:\n",
    "                result_row[f'LDA_{metric}_mean'] = lda_metrics.loc[metric, 'mean']\n",
    "                result_row[f'LDA_{metric}_ci_lower'] = lda_metrics.loc[metric, 'ci_lower']\n",
    "                result_row[f'LDA_{metric}_ci_upper'] = lda_metrics.loc[metric, 'ci_upper']\n",
    "    \n",
    "    if qda_metrics is not None:\n",
    "        for metric in ['balanced_accuracy', 'f1_macro', 'cohen_kappa']:\n",
    "            if metric in qda_metrics.index:\n",
    "                result_row[f'QDA_{metric}_mean'] = qda_metrics.loc[metric, 'mean']\n",
    "                result_row[f'QDA_{metric}_ci_lower'] = qda_metrics.loc[metric, 'ci_lower']\n",
    "                result_row[f'QDA_{metric}_ci_upper'] = qda_metrics.loc[metric, 'ci_upper']\n",
    "    \n",
    "    consensus_results.append(result_row)\n",
    "\n",
    "consensus_eval_df = pd.DataFrame(consensus_results)\n",
    "\n",
    "print(\"\\n✓ Konsensus-Ranking evaluiert\")\n",
    "print(\"\\nPerformance-Übersicht (Balanced Accuracy):\")\n",
    "print(consensus_eval_df[['n_features', 'LDA_balanced_accuracy_mean', 'QDA_balanced_accuracy_mean']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisierung: Konsensus Pareto-Kurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Pareto\n",
    "pareto_data = consensus_eval_df[['method', 'n_features', 'LDA_balanced_accuracy_mean',\n",
    "                                  'LDA_balanced_accuracy_ci_lower', 'LDA_balanced_accuracy_ci_upper']].copy()\n",
    "pareto_data.columns = ['method', 'n_features', 'mean', 'ci_lower', 'ci_upper']\n",
    "pareto_data = pareto_data.dropna()\n",
    "\n",
    "fig = plot_pareto_curve(\n",
    "    results_df=pareto_data,\n",
    "    metric='balanced_accuracy',\n",
    "    classifier='LDA (Consensus Ranking)',\n",
    "    save_path='../results/plots/phase4_consensus_pareto_lda.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature-Reduktions-Timeline (Übersicht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline-Daten\n",
    "reduction_history = [\n",
    "    {'phase': 'Phase 0', 'n_features': 261, 'description': 'Initial (3MA-X8)'},\n",
    "    {'phase': 'Phase 1', 'n_features': len(consensus_df), 'description': 'Nach Prepruning'},\n",
    "    {'phase': 'Phase 4 (50%)', 'n_features': int(len(consensus_df) * 0.5), 'description': 'Konsensus 50%'},\n",
    "    {'phase': 'Phase 4 (20%)', 'n_features': int(len(consensus_df) * 0.2), 'description': 'Konsensus 20%'},\n",
    "    {'phase': 'Phase 4 (10%)', 'n_features': int(len(consensus_df) * 0.1), 'description': 'Konsensus 10%'}\n",
    "]\n",
    "\n",
    "fig = plot_feature_reduction_timeline(\n",
    "    reduction_history=reduction_history,\n",
    "    save_path='../results/plots/phase4_reduction_timeline.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finale Empfehlung: Optimales Feature-Set\n",
    "\n",
    "Identifikation des \"Elbow-Points\" (bester Trade-off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde Elbow-Point (maximaler Gradient)\n",
    "balanced_acc = consensus_eval_df['LDA_balanced_accuracy_mean'].values\n",
    "n_features_arr = consensus_eval_df['n_features'].values\n",
    "\n",
    "# Gradients berechnen\n",
    "gradients = np.diff(balanced_acc) / np.diff(n_features_arr)\n",
    "elbow_idx = np.argmax(np.abs(gradients)) + 1  # Index des steilsten Anstiegs\n",
    "\n",
    "optimal_k = n_features_arr[elbow_idx]\n",
    "optimal_perf = balanced_acc[elbow_idx]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EMPFOHLENES FEATURE-SET (ELBOW-POINT)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Anzahl Features: {optimal_k}\")\n",
    "print(f\"Prozentsatz:     {optimal_k / n_total_features * 100:.1f}%\")\n",
    "print(f\"LDA Balanced Accuracy: {optimal_perf:.3f}\")\n",
    "print(\"\\nTop Features:\")\n",
    "top_optimal = consensus_df.head(optimal_k)\n",
    "print(top_optimal[['consensus_rank', 'feature', 'consensus_score']].head(20).to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../results/rankings', exist_ok=True)\n",
    "\n",
    "# 1. Komplettes Konsensus-Ranking\n",
    "consensus_path = '../results/rankings/phase4_consensus_ranking_full.csv'\n",
    "consensus_df.to_csv(consensus_path, index=False)\n",
    "print(f\"✓ Konsensus-Ranking gespeichert: {consensus_path}\")\n",
    "\n",
    "# 2. Top-K Konsensus-Features (optimales Set)\n",
    "optimal_features_path = '../results/rankings/phase4_optimal_features.csv'\n",
    "top_optimal.to_csv(optimal_features_path, index=False)\n",
    "print(f\"✓ Optimales Feature-Set gespeichert: {optimal_features_path}\")\n",
    "\n",
    "# 3. Evaluierungs-Ergebnisse\n",
    "os.makedirs('../results/evaluations', exist_ok=True)\n",
    "consensus_eval_path = '../results/evaluations/phase4_consensus_evaluation.csv'\n",
    "consensus_eval_df.to_csv(consensus_eval_path, index=False)\n",
    "print(f\"✓ Konsensus-Evaluierung gespeichert: {consensus_eval_path}\")\n",
    "\n",
    "# 4. Methodenvergleichs-Tabelle (alle Rankings bei optimalem K)\n",
    "comparison_data = []\n",
    "phase3_results = pd.read_csv('../results/evaluations/phase3_evaluation_master.csv')\n",
    "\n",
    "for method in phase3_results['method'].unique():\n",
    "    method_data = phase3_results[\n",
    "        (phase3_results['method'] == method) & \n",
    "        (np.abs(phase3_results['n_features'] - optimal_k) <= 2)\n",
    "    ]\n",
    "    \n",
    "    if len(method_data) > 0:\n",
    "        best_row = method_data.iloc[0]\n",
    "        comparison_data.append({\n",
    "            'Method': method,\n",
    "            'LDA_Mean': best_row.get('LDA_balanced_accuracy_mean', np.nan),\n",
    "            'LDA_CI_Lower': best_row.get('LDA_balanced_accuracy_ci_lower', np.nan),\n",
    "            'LDA_CI_Upper': best_row.get('LDA_balanced_accuracy_ci_upper', np.nan)\n",
    "        })\n",
    "\n",
    "# Konsensus hinzufügen\n",
    "consensus_row = consensus_eval_df[consensus_eval_df['n_features'] == optimal_k].iloc[0]\n",
    "comparison_data.append({\n",
    "    'Method': 'Consensus',\n",
    "    'LDA_Mean': consensus_row['LDA_balanced_accuracy_mean'],\n",
    "    'LDA_CI_Lower': consensus_row['LDA_balanced_accuracy_ci_lower'],\n",
    "    'LDA_CI_Upper': consensus_row['LDA_balanced_accuracy_ci_upper']\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).sort_values('LDA_Mean', ascending=False)\n",
    "comparison_path = '../results/evaluations/phase4_method_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"✓ Methodenvergleich gespeichert: {comparison_path}\")\n",
    "\n",
    "print(\"\\nMethodenvergleich bei optimalem K={} Features:\".format(optimal_k))\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✓ Phase 4 abgeschlossen!\n",
    "\n",
    "## Zusammenfassung der Pipeline\n",
    "\n",
    "| Phase | Eingabe | Ausgabe | Hauptziel |\n",
    "|-------|---------|---------|----------|\n",
    "| **Phase 1** | 261 Features | ~84 Features | Qualitätsfilterung & Redundanzelimination |\n",
    "| **Phase 2** | ~84 Features | 8 Rankings | Multi-Methoden Feature-Ranking (Fold-Aware) |\n",
    "| **Phase 3** | 8 Rankings | Performance-Kurven | LDA/QDA Benchmarking (10 Stufen) |\n",
    "| **Phase 4** | 8 Rankings | Konsensus-Ranking | Methodenagnostisches Core-Set |\n",
    "\n",
    "### Finale Outputs\n",
    "\n",
    "1. **Konsensus-Ranking:** `results/rankings/phase4_consensus_ranking_full.csv`\n",
    "2. **Optimales Feature-Set:** `results/rankings/phase4_optimal_features.csv`\n",
    "3. **Pareto-Plots:** `results/plots/`\n",
    "4. **Evaluierungs-Tabellen:** `results/evaluations/`\n",
    "\n",
    "---\n",
    "\n",
    "**Für Ihre Masterarbeit:**  \n",
    "Nutzen Sie die optimalen Features für weitere Experimente oder finale Klassifikation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
