{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Multi-Methoden Feature-Ranking\n",
    "\n",
    "**Masterarbeit:** Zerstörungsfreie Werkstoffprüfung mittels 3MA-X8-Mikromagnetik  \n",
    "**Input:** ~84 Features aus Phase 1  \n",
    "**Output:** 8 verschiedene Feature-Rankings\n",
    "\n",
    "---\n",
    "\n",
    "## Methodische Grundlagen\n",
    "\n",
    "### Ranking-Methoden (8)\n",
    "\n",
    "| Methode | Typ | Charakteristik |\n",
    "|---------|-----|----------------|\n",
    "| **ANOVA F-Test** | Filter (Univariat) | Schnellste Baseline, lineare Separierbarkeit |\n",
    "| **Mutual Information** | Filter (Nichtlinear) | Erfasst nichtlineare Abhängigkeiten |\n",
    "| **mRMR** | Filter (Multivariat) | Minimiert Redundanz, maximiert Relevanz |\n",
    "| **ReliefF** | Filter (Instanzbasiert) | Berücksichtigt Feature-Interaktionen |\n",
    "| **L1-Lasso** | Embedded | Sparse Lösung durch L1-Regularisierung |\n",
    "| **Random Forest** | Embedded (Nichtlinear) | Gini Importance (Mean Decrease in Impurity) |\n",
    "| **Permutation Importance** | Embedded (Modellagnostisch) | Out-of-Sample Performance-Verlust |\n",
    "| **PCA-Importance** | Unüberwacht | Loadings auf ersten Hauptkomponenten |\n",
    "\n",
    "### KRITISCH: Fold-Aware Ranking\n",
    "\n",
    "**Alle überwachten Methoden werden innerhalb 5-Fold GroupKFold CV durchgeführt!**  \n",
    "→ Ranking pro Fold, dann Mittelung über Folds (Rank/Mean)  \n",
    "→ Verhindert Overfitting-Bias\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import (\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    SelectKBest\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from mrmr import mrmr_classif\n",
    "from skrebate import ReliefF\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom Utilities\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.validation import create_group_kfold_splits\n",
    "from utils.visualization import plot_ranking_comparison\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden (Output von Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten aus Phase 1\n",
    "DATA_PATH = '../data/processed/features_after_phase1.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "TARGET_COL = 'class'\n",
    "GROUP_COL = 'sample_id'\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in [TARGET_COL, GROUP_COL]]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "groups = df[GROUP_COL].copy()\n",
    "\n",
    "print(f\"✓ Daten geladen: {X.shape}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Klassen: {y.nunique()}\")\n",
    "print(f\"  Gruppen: {groups.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing-Pipeline\n",
    "\n",
    "**KRITISCH:** Imputation und Skalierung müssen INNERHALB jedes CV-Folds erfolgen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test=None, fit=True):\n",
    "    \"\"\"\n",
    "    Preprocessing: Imputation + Standardisierung.\n",
    "    \n",
    "    WICHTIG: Nur auf Trainingsdaten fitten!\n",
    "    \"\"\"\n",
    "    if fit:\n",
    "        # Imputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train_imputed = imputer.fit_transform(X_train)\n",
    "        \n",
    "        # Scaler\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "        \n",
    "        if X_test is not None:\n",
    "            X_test_imputed = imputer.transform(X_test)\n",
    "            X_test_scaled = scaler.transform(X_test_imputed)\n",
    "            return X_train_scaled, X_test_scaled, imputer, scaler\n",
    "        else:\n",
    "            return X_train_scaled, imputer, scaler\n",
    "    else:\n",
    "        raise ValueError(\"fit muss True sein für Preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ranking-Methoden\n",
    "\n",
    "### 3.1 ANOVA F-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_anova_f(X_train, y_train, feature_names):\n",
    "    \"\"\"\n",
    "    ANOVA F-Test: Between/Within-Varianz.\n",
    "    \"\"\"\n",
    "    f_scores, _ = f_classif(X_train, y_train)\n",
    "    \n",
    "    # NaN-Handling\n",
    "    f_scores = np.nan_to_num(f_scores, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': f_scores\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_mutual_info(X_train, y_train, feature_names, random_state=42):\n",
    "    \"\"\"\n",
    "    Mutual Information: I(X;Y).\n",
    "    \"\"\"\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_train, y_train,\n",
    "        discrete_features=False,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': mi_scores\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 mRMR (Minimum Redundancy Maximum Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_mrmr(X_train, y_train, feature_names, K=None):\n",
    "    \"\"\"\n",
    "    mRMR: Maximiert Relevanz, minimiert Redundanz.\n",
    "    \"\"\"\n",
    "    if K is None:\n",
    "        K = len(feature_names)\n",
    "    \n",
    "    # Daten als DataFrame (mrmr-Bibliothek erfordert DataFrame)\n",
    "    X_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "    y_series = pd.Series(y_train, name='target')\n",
    "    \n",
    "    # mRMR berechnen\n",
    "    selected_features = mrmr_classif(X=X_df, y=y_series, K=K, show_progress=False)\n",
    "    \n",
    "    # Ranking erstellen\n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'rank': range(1, len(selected_features) + 1)\n",
    "    })\n",
    "    \n",
    "    # Fehlende Features ans Ende setzen\n",
    "    missing_features = set(feature_names) - set(selected_features)\n",
    "    if missing_features:\n",
    "        missing_df = pd.DataFrame({\n",
    "            'feature': list(missing_features),\n",
    "            'rank': range(len(selected_features) + 1, len(feature_names) + 1)\n",
    "        })\n",
    "        ranking = pd.concat([ranking, missing_df], ignore_index=True)\n",
    "    \n",
    "    # Score = inverted rank (für Konsistenz)\n",
    "    ranking['score'] = len(feature_names) - ranking['rank'] + 1\n",
    "    \n",
    "    return ranking.sort_values('rank').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ReliefF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_relieff(X_train, y_train, feature_names, n_neighbors=10):\n",
    "    \"\"\"\n",
    "    ReliefF: Instanzbasierte Feature-Gewichtung.\n",
    "    \"\"\"\n",
    "    # ReliefF erfordert Integer-Labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_train)\n",
    "    \n",
    "    # n_neighbors anpassen falls zu wenige Samples\n",
    "    n_neighbors = min(n_neighbors, len(X_train) // 2)\n",
    "    \n",
    "    relief = ReliefF(n_features_to_select=len(feature_names), n_neighbors=n_neighbors)\n",
    "    relief.fit(X_train, y_encoded)\n",
    "    \n",
    "    # Feature-Scores\n",
    "    scores = relief.feature_importances_\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': scores\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 L1-Lasso (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_lasso(X_train, y_train, feature_names, C=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    L1-Lasso: Sparse Koeffizienten.\n",
    "    \"\"\"\n",
    "    lasso = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        C=C,\n",
    "        max_iter=1000,\n",
    "        random_state=random_state,\n",
    "        multi_class='ovr'\n",
    "    )\n",
    "    \n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Absolutbeträge der Koeffizienten (über alle Klassen gemittelt)\n",
    "    coef_abs = np.abs(lasso.coef_).mean(axis=0)\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': coef_abs\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Random Forest (Gini Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_random_forest(X_train, y_train, feature_names, n_estimators=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Random Forest: Mean Decrease in Impurity (Gini).\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=random_state,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    importances = rf.feature_importances_\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': importances\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_permutation(X_train, y_train, X_val, y_val, feature_names, random_state=42):\n",
    "    \"\"\"\n",
    "    Permutation Importance: Out-of-Sample Performance-Verlust.\n",
    "    \n",
    "    KRITISCH: Berechnung auf Validierungsdaten (X_val, y_val)!\n",
    "    \"\"\"\n",
    "    # Base Model: Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=random_state,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Permutation Importance auf Validierungsdaten\n",
    "    perm_importance = permutation_importance(\n",
    "        rf, X_val, y_val,\n",
    "        n_repeats=10,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    importances = perm_importance.importances_mean\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': importances\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 PCA-Importance (Unüberwacht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_pca(X_train, feature_names, n_components=10):\n",
    "    \"\"\"\n",
    "    PCA-Importance: Loadings auf ersten Hauptkomponenten.\n",
    "    \"\"\"\n",
    "    n_components = min(n_components, X_train.shape[1], X_train.shape[0])\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_train)\n",
    "    \n",
    "    # Gewichtete Loadings (gewichtet mit explained variance)\n",
    "    loadings = np.abs(pca.components_)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    \n",
    "    weighted_loadings = np.sum(loadings * explained_var[:, np.newaxis], axis=0)\n",
    "    \n",
    "    ranking = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'score': weighted_loadings\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    ranking['rank'] = range(1, len(ranking) + 1)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fold-Aware Ranking Pipeline\n",
    "\n",
    "**KRITISCH:** Ranking innerhalb jedes CV-Folds, dann Mittelung (Rank/Mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fold_aware_rankings(X, y, groups, random_state=42, n_splits=5):\n",
    "    \"\"\"\n",
    "    Berechnet Feature-Rankings innerhalb GroupKFold CV.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: {method_name: aggregated_ranking_dataframe}\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "    gkf = create_group_kfold_splits(n_splits=n_splits)\n",
    "    \n",
    "    # Storage für Rankings pro Methode und Fold\n",
    "    all_rankings = {\n",
    "        'ANOVA': [],\n",
    "        'MutualInfo': [],\n",
    "        'mRMR': [],\n",
    "        'ReliefF': [],\n",
    "        'Lasso': [],\n",
    "        'RandomForest': [],\n",
    "        'Permutation': [],\n",
    "        'PCA': []\n",
    "    }\n",
    "    \n",
    "    print(\"Starte Fold-Aware Ranking...\\n\")\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "        print(f\"Fold {fold_idx}/{n_splits}\")\n",
    "        \n",
    "        X_train_raw, X_val_raw = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Preprocessing INNERHALB des Folds\n",
    "        X_train, X_val, imputer, scaler = preprocess_data(X_train_raw, X_val_raw, fit=True)\n",
    "        \n",
    "        # 1. ANOVA\n",
    "        print(\"  - ANOVA\")\n",
    "        rank_anova = rank_anova_f(X_train, y_train, feature_names)\n",
    "        all_rankings['ANOVA'].append(rank_anova)\n",
    "        \n",
    "        # 2. Mutual Information\n",
    "        print(\"  - Mutual Info\")\n",
    "        rank_mi = rank_mutual_info(X_train, y_train, feature_names, random_state)\n",
    "        all_rankings['MutualInfo'].append(rank_mi)\n",
    "        \n",
    "        # 3. mRMR\n",
    "        print(\"  - mRMR\")\n",
    "        rank_mrmr_result = rank_mrmr(X_train, y_train, feature_names)\n",
    "        all_rankings['mRMR'].append(rank_mrmr_result)\n",
    "        \n",
    "        # 4. ReliefF\n",
    "        print(\"  - ReliefF\")\n",
    "        rank_relief = rank_relieff(X_train, y_train, feature_names)\n",
    "        all_rankings['ReliefF'].append(rank_relief)\n",
    "        \n",
    "        # 5. Lasso\n",
    "        print(\"  - Lasso\")\n",
    "        rank_lasso_result = rank_lasso(X_train, y_train, feature_names, random_state=random_state)\n",
    "        all_rankings['Lasso'].append(rank_lasso_result)\n",
    "        \n",
    "        # 6. Random Forest\n",
    "        print(\"  - Random Forest\")\n",
    "        rank_rf = rank_random_forest(X_train, y_train, feature_names, random_state=random_state)\n",
    "        all_rankings['RandomForest'].append(rank_rf)\n",
    "        \n",
    "        # 7. Permutation Importance\n",
    "        print(\"  - Permutation\")\n",
    "        rank_perm = rank_permutation(X_train, y_train, X_val, y_val, feature_names, random_state)\n",
    "        all_rankings['Permutation'].append(rank_perm)\n",
    "        \n",
    "        # 8. PCA (unüberwacht, nur auf X_train)\n",
    "        print(\"  - PCA\")\n",
    "        rank_pca_result = rank_pca(X_train, feature_names)\n",
    "        all_rankings['PCA'].append(rank_pca_result)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Aggregation: Mittelwert der Ränge über Folds\n",
    "    print(\"Aggregiere Rankings über Folds...\\n\")\n",
    "    \n",
    "    aggregated_rankings = {}\n",
    "    \n",
    "    for method_name, fold_rankings in all_rankings.items():\n",
    "        # Sammle Ränge für jedes Feature\n",
    "        rank_dict = {feat: [] for feat in feature_names}\n",
    "        \n",
    "        for fold_ranking in fold_rankings:\n",
    "            for _, row in fold_ranking.iterrows():\n",
    "                rank_dict[row['feature']].append(row['rank'])\n",
    "        \n",
    "        # Mittelwert der Ränge\n",
    "        mean_ranks = {feat: np.mean(ranks) for feat, ranks in rank_dict.items()}\n",
    "        \n",
    "        # Final Ranking\n",
    "        final_ranking = pd.DataFrame({\n",
    "            'feature': list(mean_ranks.keys()),\n",
    "            'mean_rank': list(mean_ranks.values())\n",
    "        }).sort_values('mean_rank').reset_index(drop=True)\n",
    "        \n",
    "        final_ranking['final_rank'] = range(1, len(final_ranking) + 1)\n",
    "        \n",
    "        aggregated_rankings[method_name] = final_ranking\n",
    "    \n",
    "    return aggregated_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rankings berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAUPTBERECHNUNG\n",
    "rankings = compute_fold_aware_rankings(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    groups=groups,\n",
    "    random_state=42,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"✓ PHASE 2 ABGESCHLOSSEN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"8 Feature-Rankings berechnet (5-Fold GroupKFold CV)\")\n",
    "print(f\"Features pro Ranking: {len(rankings['ANOVA'])}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rankings anzeigen (Top 20 pro Methode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_name, ranking_df in rankings.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{method_name} - Top 20 Features\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(ranking_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisierung: Ranking-Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ranking-Vergleich (Top 20)\n",
    "rankings_for_plot = {name: df[['feature', 'mean_rank']].rename(columns={'mean_rank': 'score'}) \n",
    "                     for name, df in rankings.items()}\n",
    "\n",
    "# Wähle 3 Methoden für kompakten Plot\n",
    "selected_methods = ['ANOVA', 'RandomForest', 'mRMR']\n",
    "rankings_subset = {k: v for k, v in rankings_for_plot.items() if k in selected_methods}\n",
    "\n",
    "fig = plot_ranking_comparison(\n",
    "    rankings_dict=rankings_subset,\n",
    "    top_k=20,\n",
    "    save_path='../results/plots/phase2_ranking_comparison.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichere jedes Ranking\n",
    "import os\n",
    "os.makedirs('../results/rankings', exist_ok=True)\n",
    "\n",
    "for method_name, ranking_df in rankings.items():\n",
    "    output_path = f'../results/rankings/phase2_ranking_{method_name}.csv'\n",
    "    ranking_df.to_csv(output_path, index=False)\n",
    "    print(f\"✓ {method_name} gespeichert: {output_path}\")\n",
    "\n",
    "# Kombiniertes Ranking (alle Methoden)\n",
    "combined = pd.DataFrame({'feature': X.columns})\n",
    "for method_name, ranking_df in rankings.items():\n",
    "    combined = combined.merge(\n",
    "        ranking_df[['feature', 'final_rank']].rename(columns={'final_rank': f'rank_{method_name}'}),\n",
    "        on='feature',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "combined_path = '../results/rankings/phase2_all_rankings_combined.csv'\n",
    "combined.to_csv(combined_path, index=False)\n",
    "print(f\"\\n✓ Kombiniertes Ranking gespeichert: {combined_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✓ Phase 2 abgeschlossen!\n",
    "\n",
    "**Nächster Schritt:** Notebook 3 - Phase 3: Iterative Reduktions-Evaluierung (LDA/QDA Benchmarking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
